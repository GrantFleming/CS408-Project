\chapter{Evaluation}

\section{DSL evaluation}
\label{chapter-DSL-eval}

In section \ref{DSL} we identify three common problems with the way type
systems are currently described in literature.

\begin{enumerate}
\item Descriptions are often jargon heavy.
\item Descriptions are often lengthy with repeated information.
\item The seperate nature of the rules make dependencies non-explicit.
\end{enumerate}

In addition we prioritized readability, in particular aiming for type
definitions to be readable in a linear fashion.

We believe that in the previous section we demonstrated the concrete
ways in which we addressed the identified problems, although there
were some compromises to be made.

In order to aid readability, we chose keywords carefully in our
DSL, favouring hyphenated representations of phrases such as
"eliminated-by", "resulting-in-type" and "if". This decision has the dual
purpose of allowing the descriptions to be read in a manner somewhat
like reading a description in English while also reducing some jargon,
however this can sometimes be at the cost of addressing the second
identified problem: berevity. We feel like the the gains in
readability outway the cost in berevity here, and that the berevity is
far better addressed by the use of hierarchical structure than by
choosing shorter keywords. 

We were careful with our design to ensure flexibility with regards to
what type systems were describable. In appendix \ref{STLCspec} we show
a complete example of a variation of simply typed lambda calculus with
two base types and product types. In appendix \ref{SystemFspec} we
demonstrate how we can encode arbitrary polymorphic terms by wrapping
them in a value that is first eliminated by a type. Finally in
\ref{DTLCspec} we demonstrate that the DSL is readily used for
dependent types by describing singleton types and dependent
functions. The inclusion of a dependently typed example allows us to
discharge the acceptance criteria where we specified that our solution
must be capable of representing both non-dependent and dependent types
so long as the implementation is consistent with the design.

Some aspect of the DSL are worth further thought as there are
certainly improvements to be had.

Firstly, the dependency on consistent ordering between elimination
type descriptions and corresponding reductions in each value is not
ideal. We identify non-explicit dependencies as an issue we wish to
address and this is a prime example of such a dependency.

Another area worth consideration is the notable absence of any way to
describe user-defined datatypes. This would certainly need addressed
in some form before the DSL was useful in any real-world application.

A final limitation of our DSL is that when we introduce a new variable
binding, the scope of that new binding begins immediately after the
variable is introduced, there is no way for us to specify an
alternative location to begin the newly weakened scope. Consequently,
we are unable to represent let-expressions in the customary manner, as
the variable name appears before the value we then bind to it meaning
that we provide the new value in the already weakened scope. I.e. for
\begin{verbatim}
let x = <some-term> in <some expression involving x>
\end{verbatim}
then x is in-scope when defining the term we plan to associate to
it. This problem appears more generally any time we wish to provide a
syntax for some kind of assignment in this manner. We can work around
this issue by simply providing the term before the binding name,
however this results in wildly unintuitive syntax, opposing decades of
convention.
\begin{verbatim}
let <some-term> = x in <some expression involving x>
\end{verbatim}

\section{Software evaluation}

We open our evaluation of the software by first revisiting the
acceptance criteria provided in section \ref{sec-acceptance}. We have
already shown that the software can be ran in the manner described by
the first criterion.

We over achieve on the second criterion by including somewhat more
helpful output that we originally considered in-scope in section
\ref{sec-scope}. The program output details the number of each kind of
rule it was able to parse from the specification file rather than just
a simple success or failure message. Similarly the program output does
not simply acknowledge whither it managed to parse the source file,
but shows the user a representation of what it managed to parse along
with any remaining input it was unable to parse. Finally, although we
only commited to a success or failure message resulting from the
type-checking process, we instead supply the user with the type of the
term provided in the source file. By providing this functionality over
and above what we commited to, we greatly improve the experience in
using the software, especially where users are able to see various
rules accumulate as they write their type-system description.

We have described in chapter \ref{chapter-testing} the test cases that
evidence our efforts in meeting the third acceptance criterion.

Finally, appendix \ref{appendix:examplespecifications} shows the
specifications that we used to test our system supports both
non-dependant and dependent types discharging our final acceptance
criterion.

Another area where we believe this project succeeds is in the
transation of various areas of meta-theory to Agda code. We manage to
capture many useful invariants in our careful use of types as
discussed in the previous chapter. This helped to ensure consistent
code quality throughout the lifetime of the project.

There are several areas that we identify that contain room for
improvements and we now give details 

\subsection{Forced explicit type information}

Although there are a variety of languages we are able to type-check
using this software, there are also certain notable limitations. We
are most obviously limited by what we are able to describe using the
DSL as is discussed in the previous section. We also require users
source code to be extremely explicit about type information. There are
no appeals to techniques such as unification in order to shorten the
syntax of the languages we describe. For example, when we eliminate
some term in our end syntax, that term cannot be a construction, it
must be a computation with a synthesizable type. This is true even if
the elimination itself is type-annotated and so we might be able to
make appeals to 'magic' in order provide in this missing type
information.

\subsection{Limited DSL validation}

There are areas where we conduct sensible checks when parsing the DSL
in order to protect the user from their own errors. We make sure that
when a user supplies some pattern that we only use it in particular
rules if the premises establish complete trust. However, there are
other areas where we do not conduct checks and this leaves room for
mischief in our DSL.

Most notably, users are able to construct types that lead to circular
checks and therefore lead to a type-checking process that we cannot
guarantee to terminate. The following description is perfectly legal
in our DSL and our software will parse and attempt to use such
information.

\begin{verbatim}
type: A
  if:
    type A
\end{verbatim}

Further testing with this example specification exposes another
limitation of our system which we will explore.

\subsection{Rule matching depends on rule declaration order}

When we collect together rules of a certain type, we make no checks to
ensure that some syntax can only match zero or one of the rules. We
will discuss this further when we explore missed opportunities for
mathematical structure however what we expose here is that where there
are multiple rules that might match, the rule we use is the one that
is declared first. As a result, having some mistaken, but very
generic, entity declared early in the file might "catch" matches
before they reach their intended rules and such a problem could be
difficult to debug.

A fundamental decision that led to this problem was that of storing
the collections of rules as a list. The arbitrary ordering of rules in
a list does not admit any stucture as to make an alternative method of
processing obvious.

In hindsight we propose a tree-like structure where rules are stored
at leaves and nodes store the most specific pattern that matches all
the rules stored in its subtrees. A node might either yield children
for all sub-patterns giving rise to a collection of more specific
patterns, or a \emph{single} leaf containing a rule that matches on the
pattern. In this way we might eliminate the problem that some syntax
might match multiple patterns (since a node cannot contain both a leaf
and a set of child nodes) while also eliminating the arbitrary
dependency of matching on the order that certain elements are declared.

\subsection{Missed opportunities for mathematical structure}

We missed several opportunities
to capitalize on mathematical structure when writing our
code. We noted previously that we might have altered 
our definition of \emph{Pattern} to expose some applicative functor
structure which would have been a welcome addition to later
definitions. We are also aware that patterns as they are defined
presently form a lattice. A partial order on the set of patterns may
be defined.
$$
\mbox{let} \;μ(p) = \mbox{the set of all terms that match the pattern}\;p
$$
$$
p \leq q \iff \mu (p) \subseteq  \mu (q)
$$

We give algorithms for calculating the meet and join of a set of
patterns in appendix \ref{appendix-joinalgorithms} completing the
definition of a lattice. We might have then used this structure to
validate user supplied typing rules by ensuring that a piece of syntax
describing a type cannot match more than one rule. For each new user
supplied type pattern $t$ and the set all all existing type patterns $T$
this would be equivalent to ensuring
$$
\forall P \in \{t\} × T \;,\; \mu (meet \; P) = \emptyset
$$
which is easily calculable by checking whither $(meet \; P)$ contains
any $\bot$s.

There are also areas where we might have used this structure to help
us in parsing the source code as we rely heavily on parsing according
to patterns.

\subsection{Single universe limitation}

In an effort to simplify the late stage implementation of the
software, we decided that all languages described would consist of a
single universe of types. We call this universe "set" in our syntax
and we may use the universe in any place where a type might be
expected.

While there exists some parts of the implementation that we wrote
while intending for a user to describe the universes in our DSL too,
this was not a feature we ended up carrying through to the final
system due to time constraints in the late stages of implementation.

We might have thought to include the ability for the user to at least
choose the name of the universe, this might have been easier to
implement and could have been a welcome addition to the functionality
we have provided.

\subsection{Code quality}

There are areas of the project where we might find ways to
improve the quality of the code. Particularly in sections
\ref{section-normalisation} and \ref{section-checking-types} we notice
that many functions take long lists of arguments which is certainly
indicative of a particular 'code smell'. There are improvements to be
had by revisiting some sections of the code and collecting commonly
associated information under some sensible type, and perhaps more
generally reconsidering where some of these definitons life in order
to remove the need for so many arguments. For example, the function
for running a particular rule would be better defined inside the
record type of the rule itself. Another concrete example to be had is
that of the Context and Ruleset frequency being supplied together in
function defintions, it may have been easier to collect this
information under some type.

\subsection{Documentation}

We addressed our approach to documentation early in the planning stage
of the project, conveying our use of literate Agda files to produce
documented source code files as we work on our implemenation. While we
still believe this was a sensible choice for this project there are
downsides to having the documentation stuctured in this way.

Firstly, if particular information is required from the documentation
this is not readily available since the documentation is not available
pre-compiled and indexed in a particular format. As a result, finding
information would be a slow process unless the person was familiar
with the code base. Secondly, because of the amount of documentation
interspersed with code in the same file, the maintainence of these
files could become cumbersome. When making any alterations to the code
we must also make sure that we update the documentation otherwise we
might end up with a file that contains information that is out of date
even with regards to itself! This is as opposed to the scenario where a
seperate document might end up slightly behind the most up-to-date
versions of the source code, here the problem is lessened since we
might be able to determine its worthiness by cross checking the date
of the latest revision of each document.

The benefit of choosing this style of documentation regardless of
these problems is that we have a more 'lightweight' development
process that is more suited to smaller projects and shorter
timeframes, especially where we are not concerned that someone else
may have to maintain this code in future.


\section{Personal performance evaluation}

In general, the project unfolded as detailed in the project plan. We
deliver the artifacts outlined in section \ref{section-artifacts} and we
hit the milestones outlined in section \ref{section-milestones}. We
devoted much of the first semester to reading and research and started
to write the software during the Christmas break, completing the
software in early March and leaving some time for testing and verification.

During the implementation phase, we did not hit the target
dates chosen for the completion of the software and the completion of
the testing. We found the project to be substantially more difficult
that we anticipated and found ourselves lacking key skills that would
have helped us to meet the deadlines we set originally. This has
resulted in a slight decrease in code quality that is apparent in some
of the later areas of the codebase.

Firstly we over-estimated our dependently-typed programming skills and,
being somewhat new to this typing paradigm, progress was naturally
slower as thought was given as to how best use the features it
affords. Secondly we found ourselves lacking experience in the design
of functional programs. We certainly had experience of providing
individual functions however we had very little experience of
functional design at a higher level. We often found ourselves having
to rewrite sections of code that seemed sufficient at the time but
that made subsequent definitions prohibitively difficult.

There are two possible solutions that could have been employed here in
order to make better use of the time that was available. Firstly we
might have been more careful about reducing scope earlier in the
project. While we did not overly commit in defining exactly what we
would deliver, we initially included some optional areas that we might
explore if time allowed. This led us to conduct a broader range of
background research that was required when it might have been
more useful to regain some of that time to use in the implementation
and testing of the software. Secondly, we might have considered
writing the software in a language in which we were already familiar.

Although we do detail the above points as potential ways in which we
could have improved certain aspects of the project, we believe that we
were able to learn certain skills by writing the project in a
dependently typed function language. These skills would have been more
difficult to learn in other paradigms and we do not regret our choices
in this area.

Despite the issues discussed here, we did not have to compromise on
the features we chose to deliver and our final artifacts conform fully
with our acceptance criteria.
