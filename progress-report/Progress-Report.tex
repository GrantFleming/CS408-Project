\documentclass{ProgressReport}[2020/09/15]

\addbibresource{references.bib}

\title{Type-Checker Generation}
\markingscheme{Software Development Based}
\regnumber{201700435}
\author{Grant G Fleming}
\date{2020/2021}
\supervisor{Conor McBride}

\begin{document}
	\maketitle
	\tableofcontents
        
        \chapter{Aims and Objectives}

        \section{Aim}

        \begin{itemize}
          \item To design, implementation and verify a
            type-checker-generator capabale of generating a
            type-checker given a specification of a type system and a
            grammar of a language. 
        \end{itemize}
        
        \section{Objectives}

        \begin{itemize}
          \item Complete initial readings on type-theory covering
            Simply-Typed Lambda Calculus, System F, Hindley Milner,
            Martin L\"{o}f and bi-directional type-theory along with
            any other identified readings.
          \item Complete specific readings relating to the design and
            implementation of various typesystems and components thereof.
          \item Identify all elements needed to fully and
            unambigiously describe a type system.
          \item Verify sufficiency of identified elements by fully describing
            System F, HM and Martin Lof in this new framework
          \item Formalize a syntax for expressing these elements.
          \item Write a specification of a type-checker-generator.
          \item Design a type-checker-generator.
          \item Implement a type-checker-generator.
          \item Verify the implementation according to the
            specification.
          \item Evaluate the DSL and identify any limitations on the
            kind of type systems that it can represent.
        \end{itemize}
        
        \chapter{Related Work}

        \section{Background, Lambda Calculus and System F}

          Type systems, as with so many concepts in computer science,
          were born from the primordial soup that is the study of
          logic. The initial purpose of type systems was to resolve
          certain inconsistencies in an underlying logic. 
      
          Although originally proposed by Bertrand Russell in 1902 to
          resolve a paradox he himself had discovered in a formalization of
          Gottlob Frege's naive set theory in 1901 \cite{Russell1901}, much
          introductory material on type theory begins the story with
          Alonzo Church and his simply typed lambda calculus
          \cite{church1940}.
      
          Like Russell, Church was searching for a way to make his
          previously define system of logic, the lambda calculus,
          consistent. In 1940 he published his seminal paper that outlined a
          type system and added an extra layer of contraint to his
          previously defined system. Now, a term in lambda calclus is well
          formed if and only if it is typeable.
      
          A concrete, if informal, summation of Church's work here is that
          in this new system, if a lambda abstraction is applied to a term,
          the type of the term must be the same type as the binder in the
          abstraction. As a consequence, previously well formed terms which
          caused inconsitencies in the logic, such as
          $ (\lambda x.xx)(\lambda x.xx)  $
          were no longer well formed under the new system.
      
          The importance of this particular work is due to it's influence in
          the early years of programming language design that began around
          the 1950s and exploded in the 1960s. In fact, it turned out
          that this calculus could be used as what we would now
          recognise as a Turing-complete model of computation. This
          facilitated its use as a programming language where the type
          system he later imposed on it would aid programmers in
          avoiding certain classes of errors. 
      
          As the field of computer science developed in sophistication in
          the late 1960s and 1970s, the great minds of the era were hard at
          work on the next big development in type systems: parametric
          polymorphism. It was noted that some functions had common
          behaviour over differing types, and that the behaviour of these
          functions did not depend on the types themselves. This lead to
          redudent definitions such as having to define a function with the
          same behaviour multiple times, once for each type you wish to
          operate over. Under type systems akin to the simply typed lambda
          calculus, the id function given by the term $\lambda
          x_{\mathbb{N}}.x $ is only ever applicable to natural numbers. It
          is clear that there was immense benefit in being able to define
          functions such as id once and have them operate over any type.
      
          One such solution to this problem came to be known as System F
          or polymorphic lambda calculus. This system was independently
          discovered by Jean-Yves Girard and John Reynolds in 1972 and 1974
          respectively \cite{Girard1972,reynolds1974}.
      
          In lambda calculus, a single binder $\lambda$ is used to bind variables
          that range over values. In the simply typed lambda calculus, a
          lambda term of the form $\lambda x_{\alpha}.M_{\beta}$ (where
          $x_{\alpha}$ binds variables $x$ over a type $\alpha$ and
          $M_{\beta}$ is some term of type $\beta $) has type
          $\alpha\to\beta$ by modern notation. System F introduces a new
          binder $\Lambda$ that is used to bind variables that range over
          types. In this system, terms of the form $\Lambda t.M$ denotes a
          function that takes as its first argument some type and returns a
          term with all references to t replaced by that type. This function
          has type $\Delta t.M^{t}$ where $M^t$ is is the type of $M$.
      
          Under this system, we may write the id function as $\Lambda t
          . \lambda x_t . x$ and thus have it operate over any type t,
          so long as we supply it.
      
          One limitation of this system is the lack of shades of typing
          grey. There are only two states in this model of polymorphism:
          either a type variable has not been supplied to a binder,
          and thus can potentially take on any type, or the bound type
          variable is fully binded to a specific concrete type then the
          function is used at exactly that type. It can also be noted that under
          this system we are required to supply the type of the binder in
          every $\lambda$ expression, this could get quite tiresome. Perhaps
          there is use for some shades of grey in our type system, and do we
          really need to type our $\lambda$ binders?

          \section{Hindley-Milner}

            Work on what we now refer to as the "Hindley-Milner" type system began
            in the late 60s and grew to become the basis of many functional
            languages such as ML and Haskell.
            
            This system, like System F, incorporates parametric polymorphism as a
            first class feature. It does, however, deal with the gory details of
            its formalization in a very different way, allowing it to glean some
            desirable properties that System F cannot claim.
            
            Firstly, when we define polymorphic functions, we need not explictly
            bind the type variables, and thus when we use them, we need not
            explicitly provide a type parameter. Instead, Hindley introduces a
            system of \textit{type-schemes} \cite{hindley1969} (types
            that may contain quantified type variables) and defines a
            type as being a type-scheme that contains no type
            variables. Milner refers to these concepts as polytypes
            and monotypes respectively \cite{milner1978}.
            
            This has the consequence that a given term may have any number of
            type-schemes, some more general than others, some more specific
            allowing us some 'shades of grey' in our type system and allowing us
            to commit to stating more or less about our values as the situation
            requires. Hindley then presents the idea of a \textit{principle type
              scheme} (p.t.s) or the most general polytype, where all possible
            types for a given term are instances of the p.t.s. Consequently any
            type-scheme of a term can be created by performing some consistent
            substitution for the type variables in the p.t.s. 
            
            He then proves that any term for which you can deduce a type scheme,
            has a p.t.s. and it is always possible to work out what the p.t.s. is
            up to \textit{trivial instances} - instances that are equal up to the
            consistent renaming of type variables. Furthermore he
            introduces an algorithm accomplish this type inference -
            Algorithm W.
            
            The idea of being able to always infer the most general type of an
            expression is a defining feature of this type system. Let us take a
            moment to ponder the ramifications of this feature.
            
            In Hindley-Milner, there is no need for us to annotate the definitions
            in our code with type information. Since we can always
            infer the p.t.s (most general type-scheme) of expressions
            we, as programmers, need not supply this type information
            in code when we make our definitions. This can result in
            cleaner looking code in some circumstances however there
            is an argument that providing type information explicitly
            in the syntax of the language serves as important documentation.
                        
            The type inference algorithm that is detailed in the Hindley-Milner
            system is also instrumental in the way types are checked.
            
            In a system like System F, whenever we wish to apply a polymorphic
            function, we must provide the type parameter first before providing
            the value parameter so that we may type-check the provided value
            parameter. With Hindley-Milner, as a direct consequence of its type
            inference features, this is not necessary. Instead we can infer the
            most general type of the function, and check that the supplied
            argument is some instance of the functions input type. This is
            generally regarded as a good thing, less we end up with code that
            contains so much type information as to be rendered
            unreadable. \clearpage
            
            The classic example of such systems is the creation of lists in the
            standard $cons/[]$ way. In a language with a type system akin to
            System F, this may look
            something like:
            
            \begin{minted}{haskell}
              (cons Number 1 (cons Number 9 (cons Number 6 [])))  
            \end{minted}
            
            whereas if we can infer the most general type of $cons$ and
            subsequently verify that the supplied input is some instance of this
            type as in Hindley-Milner, the same expression might be written as:
            
            \begin{minted}{haskell}
              (cons 1 (cons 9 (cons 6 [])))  
            \end{minted}
            
            We can see this difference reflected in the typing rules of the
            systems by way of the following example on id:
            
            \[\begin{array}{c@{\qquad}|@{\qquad}c}
                  \mbox{Hindley-Milner}
                  &
                  \mbox{System F}
                  \\\\
                  id : \forall \alpha \cdot \alpha \to \alpha
                  &
                  id : \Delta \alpha \cdot \alpha \to \alpha
                  \\\\
                  id = \lambda x \cdot x
                  &
                  id = \Lambda t . \lambda x_t \cdot x
            \end{array} \]
            
            The type in the Hindley-Milner system can be proven by:
            
            
            \[\begin{array}{c}
            \mbox{\begin{prooftree}
                    \hypo{x : \alpha \in x : \alpha}   
                  \infer1[var]{x : \alpha \vdash x : \alpha}
                \infer1[abs]{\vdash \lambda x \cdot x : \alpha \to \alpha}
                \hypo{\alpha \notin free(\epsilon)}
               \infer2[gen]{\vdash \lambda x \cdot x : \forall \alpha \cdot
               \alpha \to \alpha}
            \end{prooftree}}
            \end{array} \]
            
            
            The equivalent type in System F can be proven by:
            
            \[\begin{array}{c}
            \mbox{\begin{prooftree}
                  \hypo{x : \alpha \in x : \alpha}
                \infer1[var]{x : \alpha \vdash x : \alpha}
               \infer1[abs]{\vdash \lambda x_t \cdot x : \alpha \to \alpha}
               \infer1[$\Delta$-abs]{\vdash \Lambda t . \lambda x_t \cdot
                 x : \Delta \alpha \cdot \alpha \to \alpha}
            \end{prooftree}}
            \end{array} \]
            
            In system F, this deduces the only type for $\Lambda t . \lambda x_t
            \cdot x$ (up to the renaming of bound variables) however note that
            because of the 'shades of grey' allowable as a result of type
            specialization and generalization in Hindley-Milner we may
            also deduce other type (schemes) for $\lambda x \cdot x$:
            
            \[\begin{array}{c}
            \mbox{\begin{prooftree}
                        \hypo{x : \alpha \to \beta \in x : \alpha \to \beta}
                    \infer1[var]{x : \alpha \to \beta \vdash x : \alpha \to \beta}   
                    \infer1[abs]{\vdash \lambda x \cdot x : (\alpha \to \beta) \to
                    (\alpha \to \beta)}
                    \hypo{\beta \notin free(\epsilon)}        
                \infer2[gen]{\vdash \lambda x \cdot x : \forall \beta \cdot
                  (\alpha \to \beta) \to (\alpha \to \beta)}
                \hypo{\alpha \notin free(\epsilon)}
               \infer2[gen]{\vdash \lambda x \cdot x : \forall \alpha . \forall
                 \beta \cdot (\alpha \to \beta) \to (\alpha \to \beta)}
            \end{prooftree}}
            \end{array} \]
            
            Where $\forall \alpha . \forall \beta \cdot
            (\alpha \to \beta) \to (\alpha \to \beta) \sqsubseteq \forall \alpha \cdot
               \alpha \to \alpha$ (meaning that $\forall \alpha . \forall \beta \cdot
            (\alpha \to \beta) \to (\alpha \to \beta)$ can be derived
               from $\forall \alpha \cdot \alpha \to \alpha$ by
               consistently substituting for $\alpha$ in its body.
            
            If you wanted a term with an equivalent type in system F you would
            need to create a new term $\Lambda t_1 . \Lambda t_2 . \lambda x_{t_1
              \to t_2} \cdot x$
            
            A full list of the typing rules for System F and Hindley-Milner are
            available in appendices \ref{appendix:sysFrules} and
            \ref{appendix:HMrules} respectively.
                    
          \section{Intuitionistic Type Theory}
        
          Intuitionistic type theory was devised by Per Martin-Löf
          \cite{martinlof1980} originally in the 1970s although he
          later revised the theory many times.

          Martin-Löf was motivated, in part, by what we now call the
          Curry-Howard isomorphism: the idea that statements in logic can be
          thought of as types, and a proof of a logical statement can
          be thought of as a program (consequently simplification of
          proofs correspond to the execution of a program)
          \cite{wadler2015}. He noted that despite this isomorphism,
          computer scientists often developed their own
          languages and type systems instead of using existing
          logics and hence his theory of Intuitionistic types was
          designed to have practical uses as both a logic and
          programming language.

          While the Curry-Howard isomorphism clearly had an existing
          foot hold, relating function types to implication for
          instance, Martin-Löf was the first to extend this to
          predicate logic by having his types depend on values,
          using dependent functions and dependent pairs to represent
          universal and existential quantification respectively.

          It was this introduction of `dependent types` that made this
          theory different from previous type theories and allowing
          the development of dependently typed languages with flexible
          and powerful type systems such as Agda \cite{norell}.

          Although Martin-Löf descibes many types in his descriptions
          of this type-system, there are but a few 'key players' which
          we will briefly describe here:

          \begin{itemize}
          \item Types of the form $( \Pi x \in A ) B(x)$ describe
            dependent functions, where the type $B(x)$ of the co-domain
            depends on the \emph{value} $x$ of the domain.
          \item Types of the form $( \Sigma x \in A ) B(x)$ describe
            dependent pairs, where the type $B(x)$ of the second
            element of the pair depends on the \emph{value} $x$ of the
            first element.
          \item Type of the form $A + B$ represent the disjoint union
            of two types $A$ and $B$
          \item Types of the form $\mathbb{N}_n$ describe types
            with finite numbers of values. For instance we could take
            $\mathbb{N}_1$ to be unit, $\mathbb{N}_2$ to be booleans
            etc.
          \item Types of the form $I (A , a , b)$ encode propositional
            equality. That is a value of this type represents proof
            of that $a \equiv b \in A$
          \item W-types are a little more complex. They have the form
            $( W x \in A ) B(x)$ and represent \emph{wellorderings} (a
            somewhat more intuitive explanation is that they represent
            well-founded trees). This type allows us to define types
            with complex structures that are inductive in nature. In fact,
            some of the types that Martin-Löf includes in his
            type-theory, such as that of Lists or $\mathbb{N}$ can be
            defined as W-types.
          \item Universes are introduces as a type of types, allowing
            the transfinite types needed to talk about a "sets of
            sets", Martin-Löf recognised the need to have this level
            of expression when reasoning about certain areas of maths
            such as category theory, and universe types are a paradox
            avoiding way of accomplishing this.
          \end{itemize}

          There are far more types described in this system that are
          listed here, however many of these types build on the types
          described, some can be defined completely in terms of the
          types mentioned.

          One of the consequence of this, more powerful, type system,
          is that unlike Hindley-Milner, we can no longer infer the
          most general type of any expression.

          Take, for example, a value of some dependent pair $(2 , [a :: b
            :: nil])$ perhaps the type of the second element is
          dependent on the first, and thus the $B$ in the type $(
          \Sigma x \in A ) B(x)$ corresponds to some function $\lambda
          n \to Vec \: X \: n$, but maybe it does not depend on the first
          argument at all and $B$ simply corresponds to $\lambda n \to
          Vec \: X \: 2$ - there is no way for us to definitively
          deduce $B$ from the value alone, or as an alternative view:
          the value could belong to many $\Sigma$ types, how should we
          choose?
  
          \section{Related Work - Research in Progress}

          This author believes he has completed the relevant
          background reading on a variety of notable type systems
          allowing him a very general background of the landscape of
          type-theory.

          With this reading conducted, time will now be focused on
          further research into areas specific to creating
          type-checkers and their various components. This involves
          some related work that will be partly summarized here but
          many of the readings described have yet not been fully
          consumed.

          \subsection{Contexts and Type Inference}

          Gundry, McBride and McKinna present research detailing an
          novel way of implementing unification and
          type-inference \cite{TypeInferenceInContext}. An area of
          particular interest is in their approach to the
          \emph{context} - traditionally used to track the types of
          term variables.

          In this work, the authors propose a system of explictly
          tracking type-variables in the context, even before they are
          bound to a type-scheme. As well as allocating types
          to these variables, the unification algorithm is also
          able to pull these type-variables leftward to scope them
          appropriately and resolve dependencies while it tries to
          resolve unification constraints. It cannot pull these
          variables rightward as it risks taking a variable out of
          scope where it is needed, potentially undoing the work of a
          previous constraint-solving step.

          This leads to an interesting consequence in their approach to
          generalization in let expressions. Now that type variables
          can exist in the context, we are able to instantiate a type
          scheme by introducing a new type variable to the context,
          and removing the necessary $\forall$ quantifier in the
          scheme. Consequently, to generalize, we can remove these
          type-variables from the context and introduce and
          appropriate $\forall$ quantifier. The authors explicily
          place a third element into the context, a marker that delimits
          generalization to an appropriate scope.

          \subsection{Syntactic Universes}

          More recently, the author has been exploring the concept of
          universe constructions in building a syntactic universe
          where we may build various mechanisms for accomplishing
          static analysis regardless of the underlying syntax of the
          language.

          The concept of a syntactic universe was first introduce by
          the author's supervisor, Conor McBride, and further general
          material on universe constructions was obtained when covered
          as a topic in an Advanced Function Programming class, taught
          by Fredrik Nordvall Forsberg.

          This idea is explored in a more in-depth context when we
          explore work detailing how we might build a universe of
          syntaxes that are both scope and type safe and abstractly
          define common elements of their semantics such as
          substitution \cite{DBLP:journals/corr/abs-2001-11001}.

          This is an area that is clearly relevant to this authors
          work, although - at the time of writing - progress has still
          to be made by the author in fully understanding the paper.

          \section{Related Work Still To Be Explored}

          After the completion of current readings, the author has
          plans to tackle several other papers that cover the
          following areas:

          \begin{itemize}
          \item An implementation of a dependently typed lambda
            calculus \cite{ATutorialImplementationOfDTLC}
          \item Formalization of bi-directional dependent type systems \cite{TypesWhoSayNi}
          \end{itemize}

          Although there are other less critical papers that the
          author may tackle if time allows.

          
        
        \chapter{Project Specification}
        \chapter{Project Plan}

          w/ regards to type-inference in context
        
          Given that the context is such a ubiqutous notion in
          type-theories, it is likely that this notion of being
          explicit about type-variables has uses extending beyond
          Hindley-Milner and this is an area that this author will explore
          further. This paper also details relevent unification and type
          inference algorithms that could be helpful in the
          development of a type-checker generator, should it be
          possible to abstract the algorithms over a universe of
          syntaxes and associated typing rules. Thought in this area
          is an ongoing process.

          w/ regards to syntactic universes

          How do I choose a syntactic universe? What qualities should
          it have?

        \chapter{Summary of Proposals}
        \section{Development Methodology}
        \section{Design}
        \section{Implementation}
        \section{Testing}
        \section{Evaluation}
	
	\input{appendices.tex}
	
	\clearpage
	\addcontentsline{toc}{chapter}{Bibliography}
	\printbibliography
	\nocite{*} % temporarily until I know what I'm using
\end{document}
