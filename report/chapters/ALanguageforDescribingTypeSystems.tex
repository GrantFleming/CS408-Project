\chapter{A Language for Describing Type Systems}


\section{What needs to be described}

In order to build a type checker for the language, the user must have
means to provide the following information:

\begin{itemize}
  \item Types
  \item Language constructs for each type, defining the 'shape' of
    all values of the associate type in weak head normal form
  \item A type-checking rule for each new language construct
  \item Eliminators
  \item Type synthesis rules for each elimination
  \item Beta reduction for each elimination
  \item Optional eta-expansion for each type
\end{itemize}

\section{What is supplied for free}

We provide means to describe a class of languages where all types
exist in a single universe. Although we could allow the user to
provide some name for the universe, we opt not to for simplicity and
instead mandate the construction "set" to mean the universe.

When a user describes types, they are not required to specify that it
is in the universe. In a single universe system there is nowhere else
these constructions could live and so we give this functionality for
free in the resulting software.

\section{The high-level structure of descriptions}

We say that the structure of a description of a type-system is as
follows: 

\begin{itemize}
    \item One or more type descriptions where each is:
      \begin{itemize}
      \item type pattern
      \item premises for determining it is a type
      \item zero or more elimination descriptions where each is:
        \begin{itemize}
        \item eliminator pattern
        \item premises for checking the type of the eliminator
        \item type expression of the whole elimination
        \end{itemize}        
      \item zero or more construction descriptions where each is:
        \begin{itemize}
        \item construction pattern
        \item premises for checking the type of the construction
        \item $\beta$-reduction expression for every elimination described
        \end{itemize}
      \item zero or one $\eta$-expansion rules
      \end{itemize}
\end{itemize}

We place additional conditions on when we are able to supply an
$\eta$-rule which we will discuss further in a future section, however
for now we focus on the important concepts of patterns and expressions
as these are critical to describing and building terms in our DSL.

\section{Representing Patterns}
\label{DSL-patterns}

In the high-level structure we make frequent reference to
patterns. This is a concept we borrow from Conor McBride's previous
work in bi-directional dependent type meta-theory
\cite{TypesWhoSayNi}. These are tools we use to describe when
arbitrary code matches a certain structure.

In order to define a pattern we require meta-level variables to
represent arbitrary subterms in a pattern. We adopt the convention
that meta-level variables are always upper case and so when we speak
of identifiers in this section, we are referring to strings of
uppercase alphabetic characters. Newly bound variables are denoted by
an identifier postfixed with a period, meta-level term variables are
simply an identifier. All meta-level variables must be separated from
the rest of the pattern by whitespace. All other characters are
assumed to be literals and are parsed as atoms except for
parenthesis. We may, for example, describe a pattern representing
lambda terms as follows: 
$$
\mbox{\textbackslash}\;  X. \; ->\; BODY}
$$

Where "\textbackslash" and "-$>$" are interpreted as atoms, "X." is
interpreted as a newly bound variable and "BODY" represents an
arbitrary sub-term. The scope of the newly bound variable is assumed
to extend rightward across the remainder of the term, if this is not
desirable then we may manipulate the structure of the tree with
parenthesis as one might expect, limiting the scope as in the
following example where "X" is not in scope in "ANOTHERSUBTERM". 
$$
\mbox{\textbackslash}\;  (X. \; ->\; BODY) \; and \; ANOTHERSUBTERM}
$$

\section{Representing Expressions}
  
An expression introduces a way for users to build terms from
patterns that we decide are trusted. When we later define the
structure of the DSL in more detail, we will make it clear what
patterns we are allowed to trust in each place where an expression is
required.  \hl{make sure this is done!} This concept is based on the
same meta-theory from which we base our patterns
\cite{TypesWhoSayNi}.

In defining an expression, we define atoms of the expression in
the same way that we did for patterns. We may place multiple elements
side by side separated by whitespace (again in the same way that we
did for our patterns) and use the same encoding for variables, newly
bound variables are encoded with an identifier postfixed with a
period. We may refer to bound variables by giving the appropriate
identifier \emph{prefixed} with a period as in the following example
that constructs the identity function:

$$
\mbox{\textbackslash}\;  X. \; ->\; \{.X\}}
$$

In this case we require that the bound variable reference is
surrounded by curly braces, this is because we are placing a
computation in a place that expects a construction. We will consider
the meaning of this more later and so we do not dwell on it now.
  
Since we require no meta-level sub-expression variables, we assume
that any identifier standing alone and un-post/prefixed is intended to
refer to a meta-level term variable of some pattern that is
in-scope. When this expression is used in some concrete scenario, all
the in-scope patterns will have been matched against terms and these
variable are then used to pull out subterms that we use to construct
the new term. The following example demonstrates the construction of
such an expression when we assume the pattern to the left of the
$\Rightarrow$ is in-scope:

$$
L \; and \; R \Rightarrow \;
\mbox{\textbackslash}\;  X. \; ->\; L \; and \; \{.X\}}
$$

We now consider the case where we refer to a sub-term that exists
under some number of binders in the pattern. If we wish to use such a
term then we must provide the substitution for all of the free
variables in the subterm. It is for this reason that such variables in
an expression can optionally be followed by a "/" and some
substitution. A complete substitution for all free-variables is
mandated in the DSL to avoid unintended variable capture and the
introduction of free variables as in the following case that may occur
if we did not perform this check:

$$
\mbox{\textbackslash}\;  X. \;
\mbox{\textbackslash} \; Y. \; ->\; M} \qquad \Rightarrow \;
  \mbox{\textbackslash}\;  X. \; ->\; \{.X\} \; and\;  M} $$

We must also devise a syntax to describe substitutions themselves,
we do so by simply giving substitutions as a list surrounded by square
brackets, each substitution separated by a comma and is given as an
expression so long as that expression represents a term with a
synthesizable type. When we later talk about how we build our
type-checker we will discuss this further, but for now it is suffice
to explain that we may synthesize the type of a free variable (by
looking up its type in the context) and so if we are to substitute
this variable for another term we should make sure that it is possible
to synthesize its type also incase this is a property that is needed
where the variable was used.

Substutions are given in order according to their de-bruijn index with
the "least local" variable leftmost in the list and the most local
variable rightmost. This list must also begin with a comma. We do not
supply an empty-list in the case that a meta-term-variable cannot
contain free variables, we simply omit it. The following example shows
how we use substitutions in this way by repeating the previous example
except that now we make the captured "X" explicit and remove the free
"Y":

$$
\mbox{\textbackslash}\;  X. \;
\mbox{\textbackslash} \; Y. \; ->\; M} \qquad \Rightarrow \;
  \mbox{\textbackslash}\;  X. \; ->\; .X \; and\;  M/[,\; .X,\; .X]} $$


In the substitution we do not require that the ".X" bound variable
references are surrounded by curly braces, as they are not being
embedded in some construction. So far we have given the various ways
in which we might build expressions that represent constructions and
how we may reference bound variables, how these are computations and
how we might embed a computation in a construction by surrounding it
in curly braces.

There are two other kinds of computations we can encode in our
expressions. We may write an expression representing a term followed
by a colon and another representing its type (both as constructions)
to encode type annotation information or a computation followed by
some whitespace and a construction to encode the elimination of the
computation. 

We give some examples of expressions below giving the in-scope patterns
to the left of the "$\Rightarrow$" separated by commas and the
expression to the right of it. The patterns are only given in this
manner to give context for the benefit of the reader, the
$\Rightarrow$ notation is not part of our DSL.

\begin{align*}
  A \; \times \; B \;\; , \;\; L \; and \; R  \;\; , \;\; \mbox{\textbackslash} \;
  X. \; -> \mbox{\textbackslash} \; Y. \; -> \; M \qquad &\Rightarrow
  \qquad \mbox{\textbackslash} Z. -> M/[,\; (L:A),\; .Z]\\
  TY \;\; , \;\; TM \;\; , \;\; E \qquad &\Rightarrow \qquad
  (TM:TY)\;\; E\\
  A \;\; , \;\; \mbox{\textbackslash} \; X. \; -> M \;\; ,\;\; atm \qquad
  &\Rightarrow \qquad \mbox{\textbackslash} \; X. \; -> ((M/[,\; .X] : A)\;\; atm)
\end{align*}

It is important to note the subtleties in scope when construction
expressions. In the last example, the "X." in the expression does not
shadow the "X." in the pattern, they exist in different scopes. In
fact the "X." in the pattern technically does not exist at all, there
is no way for us to reference it, either in the pattern itself (where
variables do not exist) or in an expression where it is not in
scope. It is merely a mental aid to construction.

\section{Representing premise}

As with patterns and expressions, the concepts of premise and premise
chains that we discuss in this section is heavily influenced by the
same source \cite{TypesWhoSayNi}.

There are four kinds of premise representable in the DSL. Premise are
listed together one after another, chained, in order to give the
conditions under which we may make some judgement. We will first
detail each type of premise before discussing what it means to chain
the premise together and how we determin the scope of a premise. We
will also later detail what is considered in-scope when defining an
expression as part of a premise, and so we gloss over this when
detailing the premises here.

The type premise is written by stating the word "type" followed by
the identifier corresponding to some meta level term variable in
scope.

$$
type \;\; SOMETERM
$$

The type check premise is written as some construction expression,
followed by "\textless -" and some identifier corresponding to a meta
level term variable in scope.

$$
someexpression \;\;<-\;\; SOMETERM
$$

The equivalence premise is written as a construction expression
followed by "=" and another construction expression.

$$
someexpression \;\;=\;\; anotherexpression
$$

The context extension premise is written as some binder identifier,
followed by ":" and a computation expression followed by "$|-$" and
another premise. I.e. it allows to to present an arbitrary premise in
the extended context.

$$
X : someexpression \;\;|-\;\; SOME \;\;<-\;\; OTHERPREMISE
$$

Here we note that the expression must be a computation incase we use
it in some place where type synthesis is required. The context
extension premise is somewhat different in that its use is mandated if
we wish to present a premise talking about some meta-level term
variable that exists under a binder in the original pattern. That is,
we are only allowed to present a term with free variables along with a
context expanded to type them. The name for the free variables is
assigned in the context expansion section of the premise.

A chain of premises is always considered in the context of some
pattern, the subject. A chain of premises also begins with some
trusted pattern, the nature of this pattern depends on were we are
using the chain of premise and will be detailed later. The subject
contains meta-level term variable which we do not yet trust, but seek
to trust, and the trusted pattern contains meta-level term variables
which we trust already. The purpose of a chain of premise is to obtain
trust in the subject by describing how to obtain trust in each of the
meta-level term variables in it. There are two ways that we can
discharge such a variable as trusted: we can state the type of the
term variable with a type check premise, or we can state that the term
variable is a type with the type premise. While we may use context
extension and equivalence checking at will, neither of these premise
discharge anything from what remains to be trusted in the subject.

At the start of a premise chain, only what is initially given as
trusted is in-scope (meaning that we may refer to its meta-level term
variables when building expressions). As we process each link in the
chain, we may potentially move something from the subject (to be
trusted) to the input (which we trust) bringing it in-scope from the
next link in the chain until the end. A chain may only end if there
remains nothing in the subject to be trusted, this is something we
enforce by construction in our software.

The initial scopes of a premise chain, that is the patterns where we
may reference meta term variables without establishing any trust,
differ depending on the purpose for which we are building the
chain. If we are supplying the premises for a new type, then we supply
nothing as initially trusted and the pattern representing the new type
is the subject that we seek to trust. If we are supplying the premises
for checking some language construction is of a certain type, then we
trust that the type as input, and with this information assumed we
seek to trust the new data constructor pattern that we take as a
subject. The type of the target is also in scope when we give the
premises that seek to trust some new eliminator we supply.

\section{$\eta$-expansion rules}
\label{DSL-eta-rules}

In order for us to perform $\eta$-expansion, there are two pieces of
information that we must have.

Firstly, we must know the top-level data constructor of a newly
$\eta$-expanded term. For this reason, we restrict $\eta$-expansion to
terms which contain only a single data constructor.

Secondly, for each argument to the data constructor, we must know the
eliminator which we might use such that when we eliminate the original
term with the eliminator, we get the term that is supplied as the
argument.

We can therefore give enough information to facilitate
$\eta$-expansion by choosing a pattern representing some value, and
giving an environment for the pattern, where an environment is
structurally identical except that it has a term everywhere the
original pattern had a term variable.

For instance, given the product type $A \; x \; B$ with eliminators
$fst$ and $snd$ which describes values $X , Y$ then we might give $fst
, snd$ to inform the construction of an $\eta$-expansion rule. Similarly,
for a function type $A \; -> \; B$ which describes values $\setminus
X. \; -> \; M$ and is eliminated by some pattern $E$ then we might
give $\setminus Y. \; -> \; Y$ to inform the construction of an
$\eta$-expansion rule which when applied to some target $T$ would
result in an $\eta$-expanded term $\setminus Y. \; -> \; (T \; Y)$.

\section{The DSL}
\label{DSL}

When designing the DSL there were three main problems that we wished to
address that often occur in the literature. The first is that
type-theory is an inherently jargon heavy topic and so here we attempt
to keep our DSL as jargon-free as possible, in particular favouring
syntax that can be read linearly in the same manner one might read a
poem. The trickiest part of describing a type-system should be
constructing the appropriate expressions, not understanding how the
high-level syntax of the DSL is layed out.

The second issue is that of berevity. For a given type, it is standard
to give a rule to acertain when it is valid as a type, another for
determining when a value is of that type, another for detemining the
type resulting from eliminating terms of this type, $\beta$-rules,
$\eta$-rules and sometimes more. Typically much information is
repeated across a set of such rules. It is for this reason we propose
a hierarchical structure in our DSL. The top level element is a "type"
which is described by a certain pattern. Any rules pertaining to this
type sit inside this top-level element and can access the same
pattern. There are similar hierarchical gains to be had elsewhere in
the DSL resulting in a concise language that we believe is still
approachable to read and understand.

The third and final problem that we seek to address is that the
freedom afforded with most methods of describing type-systems make
dependencies non-obvious. For instance, it is easy to fall into the
trap of thinking that perhaps typing some elimination depends on the
syntax of the target in some way, this is not the case. Dependencies
are made clear in our DSL using a combination of hierarchical
structure and carefully chosen ordering in which we require the
information to be given.

As previously explained, the top level definition in our DSL
associates a patterns with a type. If premises are required (that is
to say, if the pattern contains meta level term variables that we must
trust) then we add an "if:" clause, and then we list the premises,
with each premise on a line of its own. Inside some type description
we may give values of the type, again these are given as patterns as
described in \ref{DSL-patterns}.

A description of a simple base type with a single value might look as
follows:

\begin{BVerbatim}
  type: alpha
    value: a
\end{BVerbatim}

We will now outline how we could build more complex types by
describing functions in the simply typed lambda calculus. We will
present the description in four stages. The first stage is defining
the syntax of the type itself. Since our type contains meta-variables
then we must provide a means to trust these, and so premises are
required for this type.

\begin{BVerbatim}
  type: A -> B
    if:
      type A
      type B
\end{BVerbatim}

This is an area in which we evidence the jargon-reduction and
readability goals of our DSL design.

In the second stage we supply the required information to type
eliminations. Since this is not dependent on the specifics of the
actual values of a type, we give the required information for this
process here before we even define what the values could be making
this non-dependency far more clear. We define the eliminator by giving
some pattern, trusting the pattern and finally showing how we might
build the final type of the elimination with an expression.

\begin{BVerbatim}
  type: A -> B
    if:
      type A
      type B
    eliminated-by: E
      if:
        (A) <- E
      resulting-in-type: B
\end{BVerbatim}

We may give any number of eliminators here so long as when we later
give values, we give the appropriate number of reductions, one for
each of the eliminations described here, and we give them in the same
order. Such an example can be seen in the description of product types
in appendix \ref{STLCspec}.

In the third stage we define what patterns of syntax correspond to
values of this type and provide the necessary premises to identify
when this is the case, in our example we are forced to trust a single
meta-variable so we need to supply at least one premise to achieve
this.

\begin{BVerbatim}
  type: A -> B
    if:
      type A
      type B
    eliminated-by: E
      if:
        (A) <- E
      resulting-in-type: B
    value: \ X. -> M
      if:
        X : (A) |- (B) <- M
      reduces-to: M/[, E:A]
\end{BVerbatim}

Note that since we gave one eliminator earlier, we are forced to give
exactly one reduction inside the value description which we do by
supplying an expression that substitutes for the free variable in M.

Again we emphesize the continued prioritization of a jargon-free linear
reading experience and the use of hierarchy to make dependency explicity.

Finally we give the required information required to construct the
$\eta$-expansion rules as described in \ref{DSL-eta-rules} resulting in
a complete description of simple non-dependent function types.

\begin{BVerbatim}
  type: A -> B
    if:
      type A
      type B
    eliminated-by: E
      if:
        (A) <- E
      resulting-in-type: B
    value: \ X. -> M
      if:
        X : (A) |- (B) <- M
      reduces-to: M/[, E:A]
    expanded-by: \ Y. -> Y
\end{BVerbatim}

We have given this description in four stages intentionally to show
that at each stage we had a valid, parsable description of a type. We
allow types with no values as was the case after the first stage, we
allow the elimination typing rule to be supplied even if there are no
values. We then allow the description of the values of the type, and
finally we allow the addition of eta-expansion information. The
resulting DSL is quite modular in nature and this makes definitions
concise where all of the features are not required, as is the case for
the base type $alpha$ we introduced initially. This was an intentional
decision that we made to address the problems with berevity that we
identified earlier in this section.

A final point to make is that of whitespace. Indentation is irrelevant
in our DSL to allow maximum flexibility of use, we choose to indent
our descriptions in such a way as to emphesis the structure of the
language. There are some cases where new lines are required, after a
keyword (something ending with ":") or if the keyword is identifying
some kind of entity described by a pattern then the new line must
follow the pattern. Also, as previously mentioned, when listing
premises each one must appear on its own line.


\section{DSL Design Critique}

In section \ref{DSL} we identify three common problems with the way type
systems are currently described in literature.

\begin{enumerate}
\item Descriptions are often jargon heavy.
\item Descriptions are often lengthy with repeated information.
\item The seperate nature of the rules make dependencies non-explicit.
\end{enumerate}

In addition we prioritized readability, in particular aiming for type
definitions to be readable in a linear fashion.

We believe that in the previous section we demonstrated the concrete
ways in which we addressed the identified problems, although there
were some compromises to be made.

In order to aid readability, we chose keywords carefully in our
DSL, favouring hyphenated representations of phrases such as
"eliminated-by", "resulting-in-type" and "if". This decision has the dual
purpose of allowing the descriptions to be read in a manner somewhat
like reading a description in English while also reducing some jargon,
however this can sometimes be at the cost of addressing the second
identified problem: berevity. We feel like the the gains in
readability outway the cost in berevity here, and that the berevity is
far better addressed by the use of hierarchical structure than by
choosing shorter keywords. 

We were careful with our design to ensure flexibility with regards to
what type systems were describable. In appendix \ref{STLCspec} we show
a complete example of a variation of simply typed lambda calculus with
two base types and product types. In appendix \ref{SystemFspec} we
demonstrate how we can encode arbitrary polymorphic terms by wrapping
them in a value that is first eliminated by a type. Finally in
\ref{DTLCspec} we demonstrate that the DSL is readily used for
dependent types by describing singleton types and dependent
functions. The inclusion of a dependently typed example allows us to
discharge the acceptance criteria where we specified that our solution
must be capable of representing both non-dependent and dependent types
so long as the implementation is consistent with the design.

Some aspect of the DSL are worth further thought as there are
certainly improvements to be had.

Firstly, the dependency on consistent ordering between elimination
type descriptions and corresponding reductions in each value is not
ideal. We identify non-explicit dependencies as an issue we wish to
address and this is a prime example of such a dependency.

Another area worth consideration is the notable absence of any way to
describe user-defined datatypes. This would certainly need addressed
in some form before the DSL was useful in any real-world application.

