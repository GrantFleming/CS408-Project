\documentclass{ProgressReport}[2020/09/15]

\addbibresource{references.bib}

\renewcommand{\arraystretch}{1.7}

\title{Type-Checker Generation}
\markingscheme{Software Development Based}
\regnumber{201700435}
\author{Grant G Fleming}
\date{2020/2021}
\supervisor{Conor McBride}

\begin{document}

        \maketitle
	\tableofcontents
        
        \chapter{Aims and Objectives}

        \section{Aim}

        \begin{itemize}
          \item To design, implementation and verify a
            type-checker-generator capabale of generating a
            type-checker given a specification of a type system and a
            grammar of a language. 
        \end{itemize}
        
        \section{Objectives}

        \begin{itemize}
          \item Complete initial readings on type-theory covering
            Simply-Typed Lambda Calculus, System F, Hindley Milner,
            Martin L\"{o}f and bi-directional type-theory along with
            any other identified readings.
          \item Complete specific readings relating to the design and
            implementation of various typesystems and components thereof.
          \item Identify all elements needed to fully and
            unambigiously describe a type system and unify them under
            a common framework.
          \item Verify sufficiency of identified elements by fully describing
            two different type systems in the framework.
          \item Formalize a syntax for expressing these elements.
          \item Write a specification of a type-checker-generator
            including identifying areas for verification and/or
            testing. 
          \item Design and implement a type-checker-generator.*
          \item Verify the implementation according to the
            specification.
          \item Evaluate the DSL and identify any limitations on the
            kind of type systems that it can represent.
        \end{itemize}

        * The design and implementation sections have been combined
        here as the idea is that this phase will be conducted in an
        agile manner via scrum-style sprints.
        
        \chapter{Related Work}

        \section{Background, Lambda Calculus and System F}

          Type systems, as with so many concepts in computer science,
          were born from the primordial soup that is the study of
          logic. The initial purpose of type systems was to resolve
          certain inconsistencies in an underlying logic. 
      
          Although originally proposed by Bertrand Russell in 1902 to
          resolve a paradox he himself had discovered in a formalization of
          Gottlob Frege's naive set theory in 1901 \cite{Russell1901}, much
          introductory material on type theory begins the story with
          Alonzo Church and his simply typed lambda calculus
          \cite{church1940}.
      
          Like Russell, Church was searching for a way to make his
          previously define system of logic, the lambda calculus,
          consistent. In 1940 he published his seminal paper that outlined a
          type system and added an extra layer of contraint to his
          previously defined system. Now, a term in lambda calclus is well
          formed if and only if it is typeable.
      
          A concrete, if informal, summation of Church's work here is that
          in this new system, if a lambda abstraction is applied to a term,
          the type of the term must be the same type as the binder in the
          abstraction. As a consequence, previously well formed terms which
          caused inconsitencies in the logic, such as
          $ (\lambda x.xx)(\lambda x.xx)  $
          were no longer well formed under the new system.
      
          The importance of this particular work is due to it's influence in
          the early years of programming language design that began around
          the 1950s and exploded in the 1960s. In fact, it turned out
          that this calculus could be used as what we would now
          recognise as a Turing-complete model of computation. This
          facilitated its use as a programming language where the type
          system he later imposed on it would aid programmers in
          avoiding certain classes of errors. 
      
          As the field of computer science developed in sophistication in
          the late 1960s and 1970s, the great minds of the era were hard at
          work on the next big development in type systems: parametric
          polymorphism. It was noted that some functions had common
          behaviour over differing types, and that the behaviour of these
          functions did not depend on the types themselves. This lead to
          redudent definitions such as having to define a function with the
          same behaviour multiple times, once for each type you wish to
          operate over. Under type systems akin to the simply typed lambda
          calculus, the id function given by the term $\lambda
          x_{\mathbb{N}}.x $ is only ever applicable to natural numbers. It
          is clear that there was immense benefit in being able to define
          functions such as id once and have them operate over any type.
      
          One such solution to this problem came to be known as System F
          or polymorphic lambda calculus. This system was independently
          discovered by Jean-Yves Girard and John Reynolds in 1972 and 1974
          respectively \cite{Girard1972,reynolds1974}.
      
          In lambda calculus, a single binder $\lambda$ is used to bind variables
          that range over values. In the simply typed lambda calculus, a
          lambda term of the form $\lambda x_{\alpha}.M_{\beta}$ (where
          $x_{\alpha}$ binds variables $x$ over a type $\alpha$ and
          $M_{\beta}$ is some term of type $\beta $) has type
          $\alpha\to\beta$ by modern notation. System F introduces a new
          binder $\Lambda$ that is used to bind variables that range over
          types. In this system, terms of the form $\Lambda t.M$ denotes a
          function that takes as its first argument some type and returns a
          term with all references to t replaced by that type. This function
          has type $\Delta t.M^{t}$ where $M^t$ is is the type of $M$.
      
          Under this system, we may write the id function as $\Lambda t
          . \lambda x_t . x$ and thus have it operate over any type t,
          so long as we supply it.
      
          One limitation of this system is the lack of shades of typing
          grey. There are only two states in this model of polymorphism:
          either a type variable has not been supplied to a binder,
          and thus can potentially take on any type, or the bound type
          variable is fully binded to a specific concrete type then the
          function is used at exactly that type. It can also be noted that under
          this system we are required to supply the type of the binder in
          every $\lambda$ expression, this could get quite tiresome. Perhaps
          there is use for some shades of grey in our type system, and do we
          really need to type our $\lambda$ binders?

          \section{Hindley-Milner}

            Work on what we now refer to as the "Hindley-Milner" type system began
            in the late 60s and grew to become the basis of many functional
            languages such as ML and Haskell.
            
            This system, like System F, incorporates parametric polymorphism as a
            first class feature. It does, however, deal with the gory details of
            its formalization in a very different way, allowing it to glean some
            desirable properties that System F cannot claim.
            
            Firstly, when we define polymorphic functions, we need not explictly
            bind the type variables, and thus when we use them, we need not
            explicitly provide a type parameter. Instead, Hindley introduces a
            system of \textit{type-schemes} \cite{hindley1969} (types
            that may contain quantified type variables) and defines a
            type as being a type-scheme that contains no type
            variables. Milner refers to these concepts as polytypes
            and monotypes respectively \cite{milner1978}.
            
            This has the consequence that a given term may have any number of
            type-schemes, some more general than others, some more specific
            allowing us some 'shades of grey' in our type system and allowing us
            to commit to stating more or less about our values as the situation
            requires. Hindley then presents the idea of a \textit{principle type
              scheme} (p.t.s) or the most general polytype, where all possible
            types for a given term are instances of the p.t.s. Consequently any
            type-scheme of a term can be created by performing some consistent
            substitution for the type variables in the p.t.s. 
            
            He then proves that any term for which you can deduce a type scheme,
            has a p.t.s. and it is always possible to work out what the p.t.s. is
            up to \textit{trivial instances} - instances that are equal up to the
            consistent renaming of type variables. Furthermore he
            introduces an algorithm accomplish this type inference -
            Algorithm W.
            
            The idea of being able to always infer the most general type of an
            expression is a defining feature of this type system. Let us take a
            moment to ponder the ramifications of this feature.
            
            In Hindley-Milner, there is no need for us to annotate the definitions
            in our code with type information. Since we can always
            infer the p.t.s (most general type-scheme) of expressions
            we, as programmers, need not supply this type information
            in code when we make our definitions. This can result in
            cleaner looking code in some circumstances however there
            is an argument that providing type information explicitly
            in the syntax of the language serves as important documentation.
                        
            The type inference algorithm that is detailed in the Hindley-Milner
            system is also instrumental in the way types are checked.
            
            In a system like System F, whenever we wish to apply a polymorphic
            function, we must provide the type parameter first before providing
            the value parameter so that we may type-check the provided value
            parameter. With Hindley-Milner, as a direct consequence of its type
            inference features, this is not necessary. Instead we can infer the
            most general type of the function, and check that the supplied
            argument is some instance of the functions input type. This is
            generally regarded as a good thing, less we end up with code that
            contains so much type information as to be rendered
            unreadable.
            
            The classic example of such systems is the creation of lists in the
            standard $cons/[]$ way. In a language with a type system akin to
            System F, this may look
            something like:
            
            \begin{minted}{haskell}
              (cons Number 1 (cons Number 9 (cons Number 6 [])))  
            \end{minted}
            
            whereas if we can infer the most general type of $cons$ and
            subsequently verify that the supplied input is some instance of this
            type as in Hindley-Milner, the same expression might be written as:
            
            \begin{minted}{haskell}
              (cons 1 (cons 9 (cons 6 [])))  
            \end{minted}
            
            We can see this difference reflected in the typing rules of the
            systems by way of the following example on id:
            
            \[\begin{array}{c@{\qquad}|@{\qquad}c}
                  \mbox{Hindley-Milner}
                  &
                  \mbox{System F}
                  \\\\
                  id : \forall \alpha \cdot \alpha \to \alpha
                  &
                  id : \Delta \alpha \cdot \alpha \to \alpha
                  \\\\
                  id = \lambda x \cdot x
                  &
                  id = \Lambda t . \lambda x_t \cdot x
            \end{array} \]
            
            The type in the Hindley-Milner system can be proven by:
            
            
            \[\begin{array}{c}
            \mbox{\begin{prooftree}
                    \hypo{x : \alpha \in x : \alpha}   
                  \infer1[var]{x : \alpha \vdash x : \alpha}
                \infer1[abs]{\vdash \lambda x \cdot x : \alpha \to \alpha}
                \hypo{\alpha \notin free(\epsilon)}
               \infer2[gen]{\vdash \lambda x \cdot x : \forall \alpha \cdot
               \alpha \to \alpha}
            \end{prooftree}}
            \end{array} \]
            
            
            The equivalent type in System F can be proven by:
            
            \[\begin{array}{c}
            \mbox{\begin{prooftree}
                  \hypo{x : \alpha \in x : \alpha}
                \infer1[var]{x : \alpha \vdash x : \alpha}
               \infer1[abs]{\vdash \lambda x_t \cdot x : \alpha \to \alpha}
               \infer1[$\Delta$-abs]{\vdash \Lambda t . \lambda x_t \cdot
                 x : \Delta \alpha \cdot \alpha \to \alpha}
            \end{prooftree}}
            \end{array} \]
            
            In system F, this deduces the only type for $\Lambda t . \lambda x_t
            \cdot x$ (up to the renaming of bound variables) however note that
            because of the 'shades of grey' allowable as a result of type
            specialization and generalization in Hindley-Milner we may
            also deduce other type (schemes) for $\lambda x \cdot x$:
            
            \[\begin{array}{c}
            \mbox{\begin{prooftree}
                        \hypo{x : \alpha \to \beta \in x : \alpha \to \beta}
                    \infer1[var]{x : \alpha \to \beta \vdash x : \alpha \to \beta}   
                    \infer1[abs]{\vdash \lambda x \cdot x : (\alpha \to \beta) \to
                    (\alpha \to \beta)}
                    \hypo{\beta \notin free(\epsilon)}        
                \infer2[gen]{\vdash \lambda x \cdot x : \forall \beta \cdot
                  (\alpha \to \beta) \to (\alpha \to \beta)}
                \hypo{\alpha \notin free(\epsilon)}
               \infer2[gen]{\vdash \lambda x \cdot x : \forall \alpha . \forall
                 \beta \cdot (\alpha \to \beta) \to (\alpha \to \beta)}
            \end{prooftree}}
            \end{array} \]
            
            Where $\forall \alpha . \forall \beta \cdot
            (\alpha \to \beta) \to (\alpha \to \beta) \sqsubseteq \forall \alpha \cdot
               \alpha \to \alpha$ (meaning that $\forall \alpha . \forall \beta \cdot
            (\alpha \to \beta) \to (\alpha \to \beta)$ can be derived
               from $\forall \alpha \cdot \alpha \to \alpha$ by
               consistently substituting for $\alpha$ in its body.
            
            If you wanted a term with an equivalent type in system F you would
            need to create a new term $\Lambda t_1 . \Lambda t_2 . \lambda x_{t_1
              \to t_2} \cdot x$
            
            A full list of the typing rules for System F and Hindley-Milner are
            available in appendices \ref{appendix:sysFrules} and
            \ref{appendix:HMrules} respectively.
                    
          \section{Intuitionistic Type Theory}
        
          Intuitionistic type theory was devised by Per Martin-Löf
          \cite{martinlof1980} originally in the 1970s although he
          later revised the theory many times.

          Martin-Löf was motivated, in part, by what we now call the
          Curry-Howard isomorphism: the idea that statements in logic can be
          thought of as types, and a proof of a logical statement can
          be thought of as a program (consequently simplification of
          proofs correspond to the execution of a program)
          \cite{wadler2015}. He noted that despite this isomorphism,
          computer scientists often developed their own
          languages and type systems instead of using existing
          logics and hence his theory of Intuitionistic types was
          designed to have practical uses as both a logic and
          programming language.

          While the Curry-Howard isomorphism clearly had an existing
          foot hold, relating function types to implication for
          instance, Martin-Löf was the first to extend this to
          predicate logic by having his types depend on values,
          using dependent functions and dependent pairs to represent
          universal and existential quantification respectively.

          It was this introduction of `dependent types` that made this
          theory different from previous type theories and allowing
          the development of dependently typed languages with flexible
          and powerful type systems such as Agda \cite{norell}.

          Although Martin-Löf descibes many types in his descriptions
          of this type-system, there are but a few 'key players' which
          we will briefly describe here:

          \begin{itemize}
          \item Types of the form $( \Pi x \in A ) B(x)$ describe
            dependent functions, where the type $B(x)$ of the co-domain
            depends on the \emph{value} $x$ of the domain.
          \item Types of the form $( \Sigma x \in A ) B(x)$ describe
            dependent pairs, where the type $B(x)$ of the second
            element of the pair depends on the \emph{value} $x$ of the
            first element.
          \item Type of the form $A + B$ represent the disjoint union
            of two types $A$ and $B$
          \item Types of the form $\mathbb{N}_n$ describe types
            with finite numbers of values. For instance we could take
            $\mathbb{N}_1$ to be unit, $\mathbb{N}_2$ to be booleans
            etc.
          \item Types of the form $I (A , a , b)$ encode propositional
            equality. That is a value of this type represents proof
            of that $a \equiv b \in A$
          \item W-types are a little more complex. They have the form
            $( W x \in A ) B(x)$ and represent \emph{wellorderings} (a
            somewhat more intuitive explanation is that they represent
            well-founded trees). This type allows us to define types
            with complex structures that are inductive in nature. In fact,
            some of the types that Martin-Löf includes in his
            type-theory, such as that of Lists or $\mathbb{N}$ can be
            defined as W-types.
          \item Universes are introduces as a type of types, allowing
            the transfinite types needed to talk about a "sets of
            sets", Martin-Löf recognised the need to have this level
            of expression when reasoning about certain areas of maths
            such as category theory, and universe types are a paradox
            avoiding way of accomplishing this.
          \end{itemize}

          There are far more types described in this system that are
          listed here, however many of these types build on the types
          described, some can be defined completely in terms of the
          types mentioned.

          One of the consequence of this, more powerful, type system,
          is that unlike Hindley-Milner, we can no longer infer the
          most general type of any expression.

          Take, for example, a value of some dependent pair $(2 , [a :: b
            :: nil])$ perhaps the type of the second element is
          dependent on the first, and thus the $B$ in the type $(
          \Sigma x \in A ) B(x)$ corresponds to some function $\lambda
          n \to Vec \: X \: n$, but maybe it does not depend on the first
          argument at all and $B$ simply corresponds to $\lambda n \to
          Vec \: X \: 2$ - there is no way for us to definitively
          deduce $B$ from the value alone, or as an alternative view:
          the value could belong to many $\Sigma$ types, how should we
          choose?
  
          \section{Related Work - Research in Progress}

          This author believes he has completed the relevant
          background reading on a variety of notable type systems
          allowing him a very general background of the landscape of
          type-theory.

          With this reading conducted, time will now be focused on
          further research into areas specific to creating
          type-checkers and their various components. This involves
          some related work that will be partly summarized here but
          many of the readings described have yet not been fully
          consumed.

          \subsection{Contexts and Type Inference}

          Gundry, McBride and McKinna present research detailing an
          novel way of implementing unification and
          type-inference \cite{TypeInferenceInContext}. An area of
          particular interest is in their approach to the
          \emph{context} - traditionally used to track the types of
          term variables.

          In this work, the authors propose a system of explictly
          tracking type-variables in the context, even before they are
          bound to a type-scheme. As well as allocating types
          to these variables, the unification algorithm is also
          able to pull these type-variables leftward to scope them
          appropriately and resolve dependencies while it tries to
          resolve unification constraints. It cannot pull these
          variables rightward as it risks taking a variable out of
          scope where it is needed, potentially undoing the work of a
          previous constraint-solving step.

          This leads to an interesting consequence in their approach to
          generalization in let expressions. Now that type variables
          can exist in the context, we are able to instantiate a type
          scheme by introducing a new type variable to the context,
          and removing the necessary $\forall$ quantifier in the
          scheme. Consequently, to generalize, we can remove these
          type-variables from the context and introduce and
          appropriate $\forall$ quantifier. The authors explicily
          place a third element into the context, a marker that delimits
          generalization to an appropriate scope.

          \subsection{Syntactic Universes}

          More recently, the author has been exploring the concept of
          universe constructions in building a syntactic universe
          where we may build various mechanisms for accomplishing
          static analysis regardless of the underlying syntax of the
          language.

          The concept of a syntactic universe was first introduce by
          the author's supervisor, Conor McBride, and further general
          material on universe constructions was obtained when covered
          as a topic in an Advanced Function Programming class, taught
          by Fredrik Nordvall Forsberg.

          This idea is explored in a more in-depth context when we
          explore work detailing how we might build a universe of
          syntaxes that are both scope and type safe and abstractly
          define common elements of their semantics such as
          substitution \cite{DBLP:journals/corr/abs-2001-11001}.

          This is an area that is clearly relevant to this authors
          work, although - at the time of writing - progress has still
          to be made by the author in fully understanding the paper.

          \section{Related Work Still To Be Explored}

          After the completion of current readings, the author has
          plans to tackle several other papers that cover the
          following areas:

          \begin{itemize}
          \item An implementation of a dependently typed lambda
            calculus \cite{ATutorialImplementationOfDTLC}
          \item Formalization of bi-directional dependent type systems \cite{TypesWhoSayNi}
          \end{itemize}

          Although there are other less critical papers that the
          author may tackle if time allows.
       
        \chapter{Project Specification}

          \section{Overview}

          Writing type-checkers for compilers is a key task in the
          development and evolution of programming languages.

          Despite huge variety in the kinds of languages in existance,
          type-systems can often be described using the same kinds of
          things such as typing rules. The descriptions are becoming
          standardized enough for us to recognize common syntax for
          describing these aspects in literature.

          This project seeks to capitalize on this by abstracting
          enough common elements and leaving only what is necessary
          for describing a type-system up to a user. Given any
          description of a type-system in a specified format, a
          type-checker-generator should be able to generate a
          type-checker for the type system.
          
          \section{Goal}

          To design a DSL capable of describing type systems and to
          write a piece of software that, given one of these
          descriptions, is able to type-check code in the language
          it describes.

          \section{Artifacts}

          At a minimum, the finished project will consist of:

          \begin{itemize}
          \item A type-checker generator as a standalone binary
          \item At least one example specification in the DSL
          \item At least two source code examples for each
            specification, one which type-checks successfully and one
            which does not
          \item The final report
          \end{itemize}
          
          \section{Scope}

          \begin{center}
          \begin{tabular}{p{20em}|p{18em}}
            \large In Scope & \large Not In Scope \\ \hline \hline
            Notification on whither the type-checking succeeds
            or fails
            & Detailed feedback from the type-checking process \\ \hline
            Notifaction on whither or not the specification file is
            successfully parsed
            & Detailed feedback on the parsing of the specification file \\ \hline
            Example specifications and checkable code for small 'toy' languages
            & Example specifications and code for large 'real world' languages \\ \hline
            Single usable feature of 'running' the generator on a
            specification and piece of source code
            & Quality of life / usability features or other niceties \\ \hline
            High level reasoning on why the generated type-checkers
            are likely to be sound and complete
            & Formally proving the generator produces type-checkers
            that are sound and complete\\
          \end{tabular}            
          \end{center}        
          
          \section{Milestones}

          \begin{enumerate}
          \item A DSL that we are able to use to describe existing
            type checkers, used to write \emph{specifications} of type systems.
          \item A type-checker able to check types for a generic syntax
            in a syntactic universe under a \emph{fixed} set of typing rules
            where syntaxes and checkable source code is supplied hard-coded.
          \item A type-checker able to check types when full specifications
            and checkable source code is hard-coded.
          \item A type-checker  that checks types, reading
            the specification and source code from files.
          \end{enumerate}
          
          \section{Acceptance Criteria}

          \begin{itemize}
          \item The software can be ran from the command-line by
            giving a simple command such as: $$\mbox{type-check} \;
            <\emph{specification-file}> \;
            <\emph{source-code-file}>$$
          \item The software either succeds with a simple indication
            message, or fails with one of two kinds of error:
            \begin{itemize}
            \item Failure to parse specification
            \item Failure to successfully type-check source code
            \end{itemize}
          \item The software passes a fixed set of test cases (specifications and
            source codes) - succeeding, failing and showing
            appropriate errors as detailed in the test cases.
          \item The software can generate type-checkers capable of
            checking both non-dependent and dependent types.
          \end{itemize}
        
        \chapter{Project Plan}

        \section{Objective Timeline}

        \begin{center}
        \begin{vtimeline}[description={text width=11cm}, 
         row sep=3em]
        16/11/2020 & Complete initial readings on type-theory. \endlr
        30/11/2020 & Complete specific readings relating to the design and
          implementation of various type systems and components thereof.* \endlr
        07/12/2020 & Identify all elements needed to fully and
          unambigiously describe a type system. \endlr
        14/12/2020 & Verify sufficiency of identified elements by fully describing
            two different type systems in the framework. \endlr
        21/12/2020 & Formalize a syntax for expressing these elements. \endlr
        04/01/2021 & Write a specification of a type-checker-generator,
        including identifying areas for verification and/or testing. \endlr
        01/03/2021 & Design and implement a type-checker-generator. \endlr
        15/03/2021 & Verify the implementation according to the
          specification. \endlr
        22/03/2021 & Evaluate the DSL and identify any limitations on the
          kind of type systems that it can represent. \endlr
        \end{vtimeline}          
        \end{center}
        * This is the target date of completion for currently planned
        readings, it is likely that readings will be an ongoing
        process as more are identified.
        
        \section{Current Progress}

          \subsection{Areas of action}
        
          Initial readings on general type-theories have been
          completed and more specific in-depth readings on relavent
          topics are currently underway.

          As an exercise, a previously presented unification algorithm
          \cite{TypeInferenceInContext} has been adapted and
          re-written in Agda (the author's implementation language of
          choice).

          Writing has begun on various sections of the final report,
          including a review of the literature, somewhat similar to
          the "Related Work" section in this report.

          Some coding exercise have been conducted exploring the
          concept of using universe constructions to program generically.
        
          \subsection{Areas of thought}

          After reading about the idea of storing type-variables
          explicitly in the context \cite{TypeInferenceInContext} more
          thought is being given to how to best utilize this technique
          in this project.
          
          Given that the context is such a ubiqutous notion in
          type-theories, it is likely that this idea of being
          explicit about type-variables has uses extending beyond
          Hindley-Milner and this is an area that this author will explore
          further. This paper also details relevent unification and type
          inference algorithms that could be helpful in the
          development of a type-checker generator, should it be
          possible to abstract the algorithms over a universe of
          syntaxes and associated typing rules. Thought in this area
          is an ongoing process.

          Thought is also being given to the concept of syntactic
          universes. It is clear that this idea allows us to create
          our software generalizing over the kinds of syntax to which it
          will be applied. However the author would like to gain a
          deeper understanding about how best to design a syntactic
          universe. In the context of this project, how might one
          choose a sensible abstraction? What qualities should our
          universe have? What level of detail do we require, and thus
          force on our end user to specify?

          
        \chapter{Summary of Proposals}
        \section{Development Methodology}

        In this project, although the primary goal is the production
        of software, much of the work is theoretical in nature. With
        that in mind, it is proposed to split the project in three:
        initial theoretical work (such as formalizing a language to
        describe type systems), the design and implementation of the
        type-checker-generator (the software development element) and
        the verification of the software (again somewhat theoretical
        in nature as we look at soundness and completeness).
        
        In the development phase of the project, an agile approach to
        development will be employed. 

        The author will take a Scrum-style approach to the development
        process, adapting the usual teamwork-focused elements to be
        more suited to a single developer environment.

        Of the four identified milestones, the last three of these
        will make up the epics under this system, where these will be
        broken down into more granular tickets for each sprint.

        The development section of the project has been allocated 8
        weeks, this will be split into four sprints each lasting two
        weeks. 
        
        \section{Design}

        As we are working within a scrum-style agile framework, the
        design will take place in small increments within each
        sprint.

        As we are working in a functional language, there are many
        prevalent concepts that, although present in design literature
        in general, are not nearly as relavent in this context. For
        instance, object-oriented programming places an emphasis on
        the concepts of "design patterns", this is a concept discussed
        far less with regards to functional programming - some might
        argue because it is a more efficient paradigm that does not
        require them, others might argue that their use is implicit.

        One area of particular interest to the author is type driven
        design - where much of the type information is filled in
        first, and the implementation details often follow
        afterward. This style of development could aid us in making
        better use of Agda's dependent type system.

        It is hoped that with careful consideration of types, the
        author might identify areas to better verify correctness of
        implementing code by introducing invarients that guarantee desirable
        properties and reduce the number of possible incorrect
        implementations - although this is an area of research in its
        own right and is certainly not the focus of this project.
        
        \section{Testing/Verification}

        The testing and verification process is multi-faceted in
        nature.

        It is the aim of this project to utilize the type system in
        order to help verify correctness at the level of component
        functions (as described when discussing type-driven design
        above).

        In addition, high level test cases (in other paradigms
        referred to as integration tests) will be conducted by
        supplying example source code to the type-checker where the
        expected outcome is known in advance. Clearly these examples
        will be small in nature as the the expected outcome needs to
        be calculated by hand.

        Finally, we aim to explore the likelyhood that generated
        type-checkers are correct and complete by reasoning about our
        software. 
        	
	\input{appendices.tex}
	
	\clearpage
	\addcontentsline{toc}{chapter}{Bibliography}
	\printbibliography
	\nocite{*} % temporarily until I know what I'm using
\end{document}
