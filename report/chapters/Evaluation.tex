\chapter{Evaluation}

\section{DSL evaluation}
\label{chapter-DSL-eval}

In section \ref{DSL} we identify three common problems with the way type
systems are currently described in the literature.

\begin{enumerate}
\item Descriptions are often jargon-heavy.
\item Descriptions are often lengthy with repeated information.
\item The separate nature of the rules make dependencies non-explicit.
\end{enumerate}

We aimed to address these issues when designing our DSL while also
prioritizing readability, in particular aiming for type definitions
to be readable in a linear fashion.

We believe that in section \ref{DSL} we demonstrated the concrete
ways in which we addressed the identified problems, although there
were some compromises to be made.

To aid readability, we chose keywords carefully in our
DSL, favouring hyphenated representations of phrases such as
"eliminated-by" and "resulting-in-type". This decision has the dual
purpose of allowing the descriptions to be read in a manner somewhat
like reading a description in English while also reducing some jargon,
however, this can sometimes be at the cost of addressing the second
identified problem: brevity. We feel that the gains in
readability outway the cost in brevity here, and that the brevity is
far better addressed by the use of hierarchical structure than by
choosing shorter keywords. 

We were careful with our design to ensure flexibility with regards to
what type systems were describable. In appendix \ref{STLCspec} we show
a complete example of a variation of simply typed lambda calculus with
two base types and product types. In appendix \ref{SystemFspec} we
demonstrate how we can encode arbitrary polymorphic terms by wrapping
them in a value that is first eliminated by a type. Finally, in
\ref{DTLCspec} we demonstrate that the DSL is readily used for
dependent types by describing singleton types and dependent
functions. The inclusion of a dependently typed example allows us to
discharge the acceptance criteria where we specified that our solution
must be capable of representing both non-dependent and dependent types.

Some aspects of the DSL are worth further thought as there are
certainly improvements to be had.

Firstly, the dependency on consistent ordering between elimination
type descriptions and corresponding reductions in each value is not
ideal. We identified non-explicit dependencies as an issue we wished to
address and this is a prime example of such a dependency.

Another area worth our consideration is the notable absence of any way to
describe user-defined datatypes. This would certainly need to be addressed
in some form before the DSL was useful in any real-world application.

A final limitation of our DSL is that when we introduce a new variable
binding, the scope of that new binding begins immediately after the
variable is introduced, there is no way for us to specify an
alternative location to begin the newly weakened scope. Consequently,
we are unable to represent let-expressions in the customary manner, as
the variable name appears before the value we then bind to it meaning
that we provide the new value in the already weakened scope. I.e. for
\begin{verbatim}
let x = <some-term> in <some expression involving x>
\end{verbatim}
then x is in-scope when defining the term we plan to associate to
it. This problem appears more generally any time we wish to provide a
syntax for some kind of assignment in this manner. We can work around
this issue by simply providing the term before the binding name,
however, this results in wildly unintuitive syntax, opposing decades of
convention.
\begin{verbatim}
let <some-term> = x in <some expression involving x>
\end{verbatim}

\section{Software evaluation}

We open our evaluation of the software by first revisiting the
acceptance criteria provided in section \ref{sec-acceptance}. We have
already shown that the software can be run in the manner described by
the first criterion.

We overachieve on the second criterion by including somewhat more
helpful output that we originally considered in-scope in section
\ref{sec-scope}. The program output details the number of each kind of
rule it was able to parse from the specification file rather than just
a simple success or failure message. Similarly, the program output does
not simply acknowledge whether it managed to parse the source file
but shows the user a representation of what it managed to parse along
with any remaining input it was unable to parse. Although we
only committed to a success or failure message resulting from the
type-checking process, we instead supply the user with the type of the
term provided in the source file. By providing this functionality over
and above what we committed to, we greatly improve the experience in
using the software, especially where users can see various
rules accumulate as they write their type-system description.

We have described in chapter \ref{chapter-testing} the test cases that
evidence our efforts in meeting the third acceptance criterion.

Lastly, appendix \ref{appendix:examplespecifications} shows the
specifications that we used to test our system supports both
non-dependant and dependent types discharging our final acceptance
criterion.

As well as achieving our acceptance criteria, another area where we
believe this project succeeds is in the translation of various areas of
meta-theory to Agda code. We manage to capture many useful invariants
in our careful use of types as discussed in the previous chapter. This
helped to ensure consistent code quality throughout the lifetime of
the project. 

We have, so far, detailed some of our success in this project, but there
are several areas where we now identify shortcomings and suggest
potential improvements.

\subsection{Forced explicit type information}

Although there are a variety of languages we can type-check
using this software, there are also certain notable limitations. We
are most obviously limited by what we can describe using the
DSL as is discussed in the previous section. We also require users
source code to be extremely explicit about type information. There are
no appeals to techniques such as unification to shorten the
syntax of the languages we describe. For example, when we eliminate
some term in our end syntax, that term cannot be a construction, it
must be a computation with a synthesizable type. This is true even if
the elimination itself is type-annotated and so we might be able to
make appeals to 'magic' to provide in this missing type information.

\subsection{Limited DSL validation}

There are areas where we conduct sensible checks when parsing the DSL
to protect the user from their own errors. We make sure that
when a user supplies some pattern that we only use it in particular
rules if the premises can establish complete trust. However,
there are other areas where we do not conduct checks and this leaves
room for mischief in our DSL.

Most notably, users can construct types that lead to circular
checks and therefore lead to a type-checking process that we cannot
guarantee to terminate. The following description is perfectly legal
in our DSL and our software will parse and attempt to use such
information.

\begin{verbatim}
type: A
  if:
    type A
\end{verbatim}

Further testing with this example specification exposes another
limitation of our system which we will now explore.

\subsection{Rule matching depends on rule declaration order}

When we collect together rules of a certain type, we make no checks to
ensure that some syntax can only match zero or one of the rules. We
will discuss this further when we explore missed opportunities for
mathematical structure however what we expose here is that where
multiple rules might match, the rule we use is the one that
is declared first. As a result, having some erroneous, but very
generic, entity declared early in the file might "catch" matches
before they reach their intended rules and such a problem could be
difficult to debug.

A fundamental decision that led to this problem was that of storing
the collections of rules as a list. The arbitrary ordering of rules in
a list does not admit any structure as to make an alternative method of
processing obvious.

In hindsight we propose a tree-like structure where rules are stored
at leaves and nodes store the most specific pattern that matches all
the rules stored in its subtrees. A node might either yield children
for all sub-patterns giving rise to a collection of more specific
patterns, or a \emph{single} leaf containing a rule that matches on the
pattern. In this way, we might eliminate the problem that some syntax
might match multiple patterns (since a node cannot contain both a leaf
and a set of child nodes) while also eliminating the arbitrary
dependency of matching on the order that certain elements are declared.

\subsection{Missed opportunities for mathematical structure}

We missed several opportunities
to capitalise on mathematical structure when writing our
code. We noted previously that we might have altered 
our definition of \emph{Pattern} to expose some applicative functor
structure which would have been a welcome addition to later
definitions. We are also aware that patterns as they are defined
presently form a lattice. A partial order on the set of patterns may
be defined.
$$
\mbox{let} \;μ(p) = \mbox{the set of all terms that match the pattern}\;p
$$
$$
p \leq q \iff \mu (p) \subseteq  \mu (q)
$$

We give algorithms for calculating the meet and join of a set of
patterns in appendix \ref{appendix-joinalgorithms} completing the
definition of a lattice. We might have then used this structure to
validate user-supplied typing rules by ensuring that a piece of syntax
describing a type cannot match more than one rule. For each new
user-supplied type pattern $t$ and the set of all existing type patterns $T$
this would be equivalent to ensuring
$$
\forall P \in \{t\} × T \;,\; \mu (meet \; P) = \emptyset
$$
which is easily calculable by checking whether $(meet \; P)$ contains
any $\bot$s.

There are also areas where we might have used this structure to help
us in parsing the source code as we rely heavily on parsing according
to patterns.

\subsection{Single universe limitation}

To simplify the late-stage implementation of the
software, we decided that all languages described would consist of a
single universe of types. We call this universe "set" in our syntax
and we may use the universe in any place where a type might be
expected.

While there exist some parts of the implementation that we wrote
while intending for a user to describe the universes in our DSL too,
this was not a feature we ended up carrying through to the final
system due to time constraints.

We might have thought to include the ability for the user to at least
choose the name of the universe, this might have been easy to
implement and could have been a welcome addition to the functionality
we have provided.

\subsection{Code quality}

There are areas of the project where we might find ways to
improve the quality of the code. Particularly in sections
\ref{section-normalisation} and \ref{section-checking-types} we notice
that many functions take long lists of arguments which is certainly
indicative of a particular 'code smell'. There are improvements to be
had by revisiting some sections of the code and collecting commonly
associated information under some sensible types, and perhaps more
generally reconsidering where some of these definitions are defined
to remove the need for so many arguments. For example, the function
for running a particular rule would be better defined inside the
record type of the rule itself. Another concrete example to be had is
that Context and Ruleset are frequently supplied together in
function definitions, it may have been easier to collect this
information under some type.

\subsection{Documentation}

We addressed our approach to documentation early in the planning stage
of the project, conveying our use of literate Agda files to produce
documented source code files as we work on our implementation. While we
still believe this was a sensible choice for this project there are
downsides to having the documentation structured in this way.

Firstly, if particular information is required from the documentation
this is not readily available since the documentation is not available
pre-compiled and indexed in a particular format. As a result, finding
information would be a slow process unless the person was familiar
with the codebase. Secondly, because of the amount of documentation
interspersed with code in the same file, the maintenance of these
files could become cumbersome. When making any alterations to the code
we must also make sure that we update the documentation otherwise we
might end up with a file that contains information that is out of date
even with regards to itself! This is as opposed to the scenario where a
separate document might end up slightly behind the most up-to-date
versions of the source code, here the problem is lessened since we
might be able to determine its worthiness by cross-checking the date
of the latest revision of each document.

The benefit of choosing this style of documentation despite
these problems is that we have a more 'lightweight' development
process that is better suited to smaller projects and shorter
timeframes, especially when we are not concerned that someone else
may have to maintain this code in future.

\section{Personal performance evaluation}

In general, the project unfolded as detailed in the project plan. We
deliver the artefacts outlined in section \ref{section-artifacts} and we
hit the milestones outlined in section \ref{section-milestones}. We
devoted much of the first semester to reading and research and started
to write the software during the Christmas break, completing the
software in early March and leaving some time for testing and verification.

During the implementation phase, we did not hit the target
dates chosen for the completion of the software and the completion of
the testing. We found the project to be substantially more difficult
than we anticipated and found ourselves lacking key skills that would
have helped us to meet the deadlines we set originally. This has
resulted in a slight decrease in code quality that is apparent in some
of the later areas of the codebase.

Firstly we over-estimated our dependently-typed programming skills and,
being somewhat new to this typing paradigm, progress was naturally
slower as thought was given as to how best to use the features it
affords. Secondly, we found ourselves lacking experience in the design
of functional programs. We certainly had experience in providing
individual functions however we had very little experience of
functional design at a higher level. We often found ourselves having
to rewrite sections of code that seemed sufficient at the time but
that made subsequent definitions prohibitively difficult.

Two possible solutions could have been employed here
to make better use of the time that was available. Firstly we
might have been more careful about reducing scope earlier in the
project. While we did not over commit in defining exactly what we
would deliver, we initially included some optional areas that we might
explore if time allowed. This led us to conduct a broader range of
background research that was required when it might have been
more useful to regain some of that time to use in the implementation
and testing of the software. Secondly, we might have considered
writing the software in a language in which we were already familiar.

Although we do detail the above points as potential ways in which we
could have improved certain aspects of the project, we believe that we
were able to learn new skills by writing the project in a
dependently typed function language despite being less familiar with
this paradigm. These skills would have been more difficult to learn
in other paradigms and we do not regret our decision.

We have discussed many issues in our evaluation, however, these issues
did not necessitate any compromise on the features we committed to delivering
and our final artefacts conform fully with our acceptance criteria.
